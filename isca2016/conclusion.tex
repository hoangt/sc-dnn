\section{Conclusion}
\label{sec:conclude}

In this paper we proposed hardware optimizations for exploiting computation and data sparsity in DNN training to improve training performance.  We provide empirical evidence that non-trivial amounts of sparsity exists in real-world DNN training workloads.  Our technique includes low-cost extensions to the processor and memory systems for efficiently leverage sparsity optimization opportunities in DNN training.  Our evaluation shows that our hardware optimizations enable up to a $3X$ speedup for real-world training workloads compared to a baseline implementation that is vectorized and loop unrolled.  Our optimizations are up to $1.5X$ faster a software approach to exploiting sparsity optimization opportunities. 