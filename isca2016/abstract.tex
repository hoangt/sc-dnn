\begin{abstract}
Deep learning has recently emerged as an important machine learning approach, with big deep neural networks (DNN) models trained on vast amounts of data demonstrating state-of-the-art accuracy on important yet challenging artificial intelligence tasks, such as image and speech recognition.  However, training big DNN models using large training data is both compute and memory intensive.

This paper proposes hardware techniques for improving system performance and scalability for DNN training workloads by exploiting the sparse nature of computation and data to reduce the compute and memory requirements.  Our techniques improve training efficiency by avoiding resource utilization on sparse data values (i.e., zeroes) which do not impact training quality. Our design is transparent to software, enabling existing codes to enjoy a performance boost without software modifications. 

Evaluation on real and simulated hardware using real-world image recognition workloads shows that our low-cost techniques can significantly improve DNN training performance and outperform state-of-the-art software approaches.

\end{abstract}
