\section{Sparsity in DNN Training}
 \label{sec:sparse_dnn_training}
 
 In this section we motivate our work and project the benefits of our optimizations for DNN training.  First, we provide some intutition of why sparsity exists in DNN training.  Next, using an image recogntion workload, we empirically demonstrate the amount of sparsity that exists in practice.  Finally, we discuss how available sparsity can be exploited using our techniques to improve training efficiency. 
 
\subsection{Sources of Sparsity}
\label{subsec:sparsity_source}

Machine learning experts have long observed that DNN training using back-propagation and gradient descent involves a considerable amount of computations on sparse data structures~\cite{Ng04, Nair10, Krizhevsky12, Bengio13, Srivastava14a}.  Specifically, performance-critical data of training, such as error gradients and weight deltas, can exhibit noticeable levels of sparsity during training.   Some of the sparsity arise naturally from the training algorithm  and the underlying matrix multiplication kernels.  For example, correct predictions of a neuron's output activation during feed-forwad evaluation results in zero-valued neuron error terms, during back-propagation, which can introduce sparsity in the rest of the network.  Beyond this, standard techniques for boosting training quality often introduce additional sparsity in the network.  These include techniques such as Rectified Linear Units (ReLUs)~\cite{Nair10, Krizhevsky12} for faster convergence, and  L$_1$~\cite{Ng04, Bengio13} and Dropout~\cite{Srivastava14a} regularization methods for reducing overfitting.  {\color{red} Trishul to help with this content}
 
\subsection{Sparsity in real-word image recognition}
\label{subsec:sparsity_profile}

For a better insight into the amount of sparsity that exists in real-world DNN training workloads, we profile the training of a DNN model on the standard {\it CIFAR-10} image recognition task~\cite{KrizhevskyThesis} (described in ~\ref{subsec:eval_method}).  In our study, we reason about sparsity from $2$ perspectives: (i) computation sparsity and (ii)  data sparsity.  Computation sparsity measures the percentage of mutiply-add operations that are performed on zero values in the performance-critical phases of training (e.g., feed-forward evaluation), while data sparsity measures the percentage of zeroes in the input data (e.g., activations) of these phases.  Both perspectives are useful because they capture different impacts of sparsity on system performance, and motivate different optimization opportunities.  Computation sparsity captures the impact on processing cycles, data sparsity captures the impact on memory capacity, and both metrics capture the impact on memory bandwidth.  We measure both sparsity metrics over $10$ training epochs using the standard training data set of $60000$ images. 

\begin{figure*}
 \centering
 \includegraphics[width=1.9\columnwidth]{Figures/multi_computesparsity.png}
\caption{Computation sparsity in CIFAR-10 training.}
 \label{fig:cifar-10_compute_sparsity}
 \end{figure*}
 
\subsubsection{Computation Sparsity.}
 Figure~\ref{fig:cifar-10_compute_sparsity} reports computation sparsity in the key phases of DNN training for the first $10$ epochs of training a CIFAR-10 model.  Since computation sparsity represents opportunities to safely reduce processing cycles, Figure~\ref{fig:cifar-10_compute_sparsity} reports sparsity for different computation granularities to show the potential benefits for non-vectorized (i.e., {\it Word}) and vectorized (e.g., {\it 4-Words}) implementations of the computation kernels.  For example, for feed-forward evaluation, {\it Word} sparsity is the percentage of CPU multiply-adds that can be skipped because one of the input activation or weight values is zero, while {\it 4-Words} sparsity is the percentage of 4-wide vector multiply-adds that can be skipped because either the $4$ activation values or $4$ weight values are zero. 
 
We make the following four observations regarding computation sparsity in DNN training from Figure~\ref{fig:cifar-10_compute_sparsity}.  First, considerable sparsity exists in all the training phases:{\it Word} sparsity is ($29\%$---$43\%$) for feed-forward evalution, ($73\%$---$84\%$) for backpropagation, and ($77\%$--$92\%$) for weight updates.   Second, the training phases have different amounts of computation sparsity, with feed-forward evaluation having the least amount of computation sparsity. Third, computation sparsity generally increases with epoch count.  Fourth, the impact of vectorization on computation sparsity varies across the training phases: no impact for backpropagation, modest sparsity reduction for weight updates, and significant sparsity reduction for feed-forward evalution (e.g., {\it 4-Words} sparsity is half of {\it Word}).  In summary, the results shows there is potential to signifcantly reduce the processing and bandwidth requirements of DNN training by exploiting computation sparsity, but vectorization limits the benefits for feed-forward evaluation. 
  
 \begin{figure*}
 \centering
 \includegraphics[width=1.9\columnwidth]{Figures/multi_datasparsity.png}
\caption{Data sparsity in CIFAR-10 training.}
 \label{fig:cifar-10_data_sparsity}
 \end{figure*}

\subsubsection{Data Sparsity.} 
Figure~\ref{fig:cifar-10_data_sparsity} reports the sparsity of the different performance-critical data in DNN training: (i) activations, (ii) error gradients, (iii) weight deltas, and (iv) weights.  The results are presented for word and cacheline granularities.  Data sparsity at word granularity is the percentage of individual data values (e.g., activations) which are zeroes, while cacheline granularity is the percentage of data cache lines containing only zeroes.  Since training data values are represented with 4-byte floats, cacheline granularity represents clusterings of sparse data values of size $16$.  Viewing data sparsity at both word and cacheline granularities helps to understand the trade-offs of different optimization strategies, since significantly more expensive hardware is required to track sparsity at word granularity compared to cacheline granularity.  

We can make the following three observations regarding data sparsity in DNN training from Figure~\ref{fig:cifar-10_data_sparsity}.  First, sparsity levels varies across the different data items: at word granularity, weights have negligible sparsity, while activations ($26\%$---$33\%$), error gradients ($83\%$---$85\%$), and weight deltas ($66\%$---$83\%$) are considerably sparse.   Second, for the sparse data items, sparsity tends to increase with more training epochs.  Third, sparsity levels are reduced at cacheline granularity to about half of word granularity, which suggests poor clustering of sparse data values.  However, considerable levels of sparsity still remains at the cacheline granularity.  In summary, the results show that cache capacity and bandwidth requirements of DNN training can be significantly reduced by exploiting data  sparsity, even at cacheline granularity.

\begin{figure}
 \centering
 \includegraphics[width=.9\columnwidth]{Figures/deltas_source_code.png}
\caption{Code snippet for computing weight deltas.}
 \label{fig:deltas_source_code}
 \end{figure}

\subsection{Optimization Opportunities}
\label{subsec:sparse_code_oppor}
We motivate our hardware optimizations for sparse data computations in training using a performance-critical kernel for computing weight deltas during back-propagation.  Figure~\ref{fig:deltas_source_code} illustrates a simplified version of this kernel\footnote{Our approach also applies to vectorized (e.g., SIMD) kernels, but we use the simple forms in our discussion for convenience.}.   The weight deltas of a layer in the DNN are computed by the inner product of the neuron activations and error gradients.  Given the amounts of computation and data sparsity in training, a promising optimization for this kernel is to skip the multiply-add operations if \emph{activations[i]} or \emph{errors[j]} is zero.  Moreover,  if \emph{errors[j]} is zero, the inner loop can be skipped entirely since \emph{errors[j]} is loop-invariant. These optimization ideas also apply to the other performance-critical kernels of training, e.g., feed-forward evalution. 

Although these optimizations could be implemented in software by checking the data values for zero, that approach has a couple of practical limitations.   First, it requires software changes which might not be possible for existing binaries.   Second, the required software checks incur both compute and memory overheads, which could be significant and outweigh the optimization benefits.  For example, checking \emph{activations[i]} and checking \emph{errors[j]} have different performance impacts.  Checking \emph{errors[j]} is likely to be beneficial because it can be done outside the inner loop, and helps to skip large amounts of computation. In contrast, checking \emph{activations[i]} will likely hurt performance because it occurs inside the inner loop, and can save only a small amount of computation. Our hardware optimizations  avoid these limitations.  We compare the performance benefits of software and hardware  approaches in our evaluation. 



